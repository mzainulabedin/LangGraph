{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph, A very basic example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function3(input3):\n",
    "    return input3\n",
    "\n",
    "def function1(input1):\n",
    "    return input1 + \"from first function,\"\n",
    "\n",
    "def function2(input2):\n",
    "    output=function3(\"this is third in between,\")\n",
    "    return input2 + \" \" + output + \" and greetings from second function,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow1=Graph()\n",
    "\n",
    "workflow1.add_node(\"function1\", function1)\n",
    "workflow1.add_node(\"function2\",function2)\n",
    "\n",
    "workflow1.add_edge(\"function1\",\"function2\")\n",
    "\n",
    "workflow1.set_entry_point(\"function1\")\n",
    "workflow1.set_finish_point(\"function2\")\n",
    "\n",
    "app1=workflow1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app1.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app1.invoke(\"hi this is Zain \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"hi this is Zain \"\n",
    "\n",
    "for output in app1.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets create workflow with llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callingLLM(state: MessagesState):\n",
    "    #print(f\"messages: {state[\"messages\"]}\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response=llm.invoke(state[\"messages\"]).content\n",
    "    #print(f\"response: {response}\")\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def textToUpperCase(state: MessagesState):\n",
    "    #print(f\"messages: {state[\"messages\"]}\")\n",
    "    upper_case=state[\"messages\"][-1].content.upper()\n",
    "    return {\"messages\": upper_case}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "workflow2=StateGraph(state_schema=MessagesState)\n",
    "\n",
    "workflow2.add_node(\"llm\",callingLLM)\n",
    "workflow2.add_node(\"upper_string\",textToUpperCase)\n",
    "workflow2.add_edge(\"llm\",\"upper_string\")\n",
    "\n",
    "workflow2.set_entry_point(\"llm\")\n",
    "workflow2.set_finish_point(\"upper_string\")\n",
    "\n",
    "app2=workflow2.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app2.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# res = app2.invoke({\"messages\": \"My name is Zain, What are you?\"}, config)\n",
    "# res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = app2.invoke({\"messages\": \"what is my name?\"}, config)\n",
    "# res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"My name is Zain, what is a name of first Pakistan prime minister? give me only name.\"\n",
    "\n",
    "for output in app2.stream({\"messages\": input}, config):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value[\"messages\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"what is my name? give me only name\"\n",
    "\n",
    "for output in app2.stream({\"messages\": input}, config):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value[\"messages\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating a RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the AgentState type\n",
    "class AgentState(TypedDict):\n",
    "    question : str\n",
    "    question_type : str\n",
    "    answer : str\n",
    "    documents : list[object]\n",
    "    vector_store : None\n",
    "    retriever : None\n",
    "    json_content : str\n",
    "    is_valid_json : bool\n",
    "    \n",
    "\n",
    "# defining load_document function\n",
    "def load_document(agentState: AgentState) -> AgentState: \n",
    "    print(\"---DOCUMENT LOAD STARTED---\")\n",
    "    # Specify the root directory where you want to search for PDF files\n",
    "    filePath = \"EcoDocs.pdf\"\n",
    "\n",
    "    # Initialize an empty list to store loaded documents\n",
    "    docs = []\n",
    "\n",
    "    # load a single PDF file\n",
    "    print(filePath)\n",
    "    pdf_loader = PyPDFLoader(filePath)\n",
    "    docs = pdf_loader.load()\n",
    "\n",
    "    agentState['documents'] = docs\n",
    "    print(\"---DOCUMENT LOAD COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining split_text function\n",
    "def split_text(agentState: AgentState) -> AgentState: \n",
    "    print(\"---TEXT SPLITTING STARTED---\")\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    documents = agentState['documents']\n",
    "    new_docs = text_splitter.split_documents(documents=documents)\n",
    "    # doc_strings = [doc.page_content for doc in new_docs]\n",
    "    agentState['documents'] = new_docs\n",
    "    print(\"---TEXT SPLITTING COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining create_vectorStore function\n",
    "def create_vectorStore(agentState: AgentState) -> AgentState: \n",
    "    print(\"---VECTOR DB CREATION STARTED---\")\n",
    "    documents = agentState['documents']\n",
    "\n",
    "    if os.path.exists(\"ecodoc_faissdb\"):\n",
    "        vector_store = FAISS.load_local(folder_path= \"ecodoc_faissdb\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        vector_store.save_local(\"ecodoc_faissdb\")\n",
    "    agentState['vector_store'] = vector_store\n",
    "    print(\"---VECTOR DB CREATION COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining create_retriever function\n",
    "def create_retriever(agentState: AgentState) -> AgentState:\n",
    "    print(\"---RETRIEVER STARTED---\") \n",
    "    vector_store = agentState['vector_store']\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    agentState['retriever'] = retriever\n",
    "    print(\"---RETRIEVER COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining get_answer function\n",
    "def get_answer(agentState: AgentState) -> AgentState: \n",
    "    print(\"---LLM CALLING STARTED---\")\n",
    "    question = agentState['question']\n",
    "    documents = agentState['documents']\n",
    "    \n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. When asked a question, provide the most relevant information from the provided documents.\n",
    "    If the information is not available, respond that there is no such information in the system files,\n",
    "    and suggest contacting the helpdesk for further assistance. \n",
    "    Do not use any other information than given vector store to provide answers.  \n",
    "    documents: {documents}\n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  {\"documents\": lambda x: documents, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    generation = rag_chain.invoke(question)\n",
    "\n",
    "    agentState['answer'] = generation\n",
    "    print(\"---LLM CALLING COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow4 = StateGraph(AgentState)\n",
    "workflow4.add_node(\"load_document\", load_document)\n",
    "workflow4.add_node(\"split_text\", split_text)\n",
    "workflow4.add_node(\"create_vectorStore\", create_vectorStore)\n",
    "workflow4.add_node(\"create_retriever\", create_retriever)\n",
    "workflow4.add_node(\"get_answer\", get_answer)\n",
    "workflow4.add_edge('load_document', 'split_text')\n",
    "workflow4.add_edge('split_text', 'create_vectorStore')\n",
    "workflow4.add_edge('create_vectorStore', 'create_retriever')\n",
    "workflow4.add_edge('create_retriever', 'get_answer')\n",
    "workflow4.set_entry_point(\"load_document\")\n",
    "workflow4.set_finish_point(\"get_answer\")\n",
    "app4 = workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app4.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# defining AgentState Object\n",
    "inputs = AgentState(\n",
    "    question = \"Let me know about base field?\"\n",
    ")\n",
    "\n",
    "output = app4.invoke(inputs)\n",
    "\n",
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "# %pip install jsonschema\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def deciding_question_type(agentState: AgentState) -> AgentState:\n",
    "    print(\"---LLM CALLING TO DECIDE QUESTION TYPE STARTED---\")\n",
    "    question = agentState['question']\n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. When asked a question, you need to decide if it is question our user asking to create a\n",
    "    a form .\n",
    "\n",
    "    create form means user actually want to create a dynamic input form a page in the application.\n",
    "    following are examples of create form: \n",
    "    example 1: create a form for employee information\n",
    "    example 2: create a form where in can store crop field attributes\n",
    "    example 3: create a page for customer's review feedback\n",
    "\n",
    "    you need to decide which type of question user is asking and then return one of following enum and nothing else.\n",
    "    question\n",
    "    creation\n",
    "    \n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  {\"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    question_type = rag_chain.invoke(question)\n",
    "    agentState['question_type'] = question_type\n",
    "    print(\"---LLM CALLING TO DECIDE QUESTION TYPE COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n",
    "\n",
    "def get_form_fields(agentState: AgentState) -> AgentState:\n",
    "    print(\"---LLM CALLING TO GENERATE FIELD STARTED---\")\n",
    "    question = agentState['question']\n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. user ask you to create a form for a specific purpose. you need to create columns\n",
    "    and their attributes related to the form user ask to create. you need to return a list of columns and their attributes\n",
    "    you need to generate your answer in json format. column types must be one of following \n",
    "    'string', 'integer', 'date', 'decimal'\n",
    "    return only json and nothing else.\n",
    "\n",
    "    following is an example of form fields in json format:\n",
    "\n",
    "    {{\n",
    "        'columns': [\n",
    "            {{\n",
    "                'columnName': 'Name',\n",
    "                'columnType': 'string',\n",
    "                'textLength': 50,\n",
    "                'required'   : true\n",
    "            }},\n",
    "            {{\n",
    "                'name': 'date_of_birth',\n",
    "                'type': 'date',\n",
    "                'required': false\n",
    "            }},\n",
    "            {{\n",
    "                'name': 'phone',\n",
    "                'type': 'integer',\n",
    "                'required': true\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  { \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    json_content = rag_chain.invoke(question)\n",
    "\n",
    "    agentState['json_content'] = json_content\n",
    "    # print(json_content )\n",
    "    print(\"---LLM CALLING TO GENERATE FIELD COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n",
    "\n",
    "def validate_json(agentState: AgentState) -> AgentState:\n",
    "    print(\"---VALIDATE JSON STARTED---\")\n",
    "    json_content = agentState['json_content']\n",
    "    # load json schema from file 'json-schema.json'\n",
    "    with open('json-schema.json') as f:\n",
    "        schema = json.load(f)\n",
    "    # validate json content against schema\n",
    "    try:\n",
    "        validate(instance=json.loads(json_content), schema=schema) \n",
    "        agentState['is_valid_json'] = True\n",
    "    except Exception as e:\n",
    "        agentState['json_content'] = \"Invalid JSON content\"\n",
    "        agentState['is_valid_json'] = False\n",
    "    print(f\"---VALIDATE JSON COMPLETED : {agentState['is_valid_json']}---\")\n",
    "    return agentState\n",
    "\n",
    "def decide_to_regenerate(agentState: AgentState) -> Literal[\"get_form_fields\", \"return_answer\"]:\n",
    "    is_valid_json = agentState['is_valid_json']\n",
    "    if is_valid_json:\n",
    "        return \"return_answer\"\n",
    "    else:\n",
    "        return \"get_form_fields\"\n",
    "\n",
    "def decide_to_reg(agentState: AgentState) -> Literal[\"load_document\", \"get_form_fields\"]:\n",
    "    question_type = agentState['question_type']\n",
    "    if question_type == \"question\":\n",
    "        return \"load_document\"\n",
    "    else:\n",
    "        return \"get_form_fields\"\n",
    "\n",
    "def return_answer(agentState: AgentState) -> AgentState:\n",
    "    return agentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow5 = StateGraph(AgentState)\n",
    "\n",
    "workflow5.add_node(\"load_document\", load_document)\n",
    "workflow5.add_node(\"split_text\", split_text)\n",
    "workflow5.add_node(\"create_vectorStore\", create_vectorStore)\n",
    "workflow5.add_node(\"create_retriever\", create_retriever)\n",
    "workflow5.add_node(\"get_answer\", get_answer)\n",
    "workflow5.add_node(\"deciding_question_type\", deciding_question_type)\n",
    "workflow5.add_node(\"get_form_fields\", get_form_fields)\n",
    "workflow5.add_node(\"validate_json\", validate_json)\n",
    "workflow5.add_node(\"decide_to_regenerate\", decide_to_regenerate)\n",
    "workflow5.add_node(\"return_answer\", return_answer)\n",
    "workflow5.add_node(\"decide_to_reg\", decide_to_reg)\n",
    "\n",
    "#workflow5.add_edge('deciding_question_type', 'decide_to_reg')\n",
    "workflow5.add_conditional_edges('deciding_question_type', decide_to_reg)\n",
    "# workflow5.add_conditional_edges('decide_to_reg', decide_to_reg)\n",
    "workflow5.add_edge('get_form_fields', 'validate_json')\n",
    "workflow5.add_conditional_edges('validate_json', decide_to_regenerate)\n",
    "workflow5.add_edge('load_document', 'split_text')\n",
    "workflow5.add_edge('split_text', 'create_vectorStore')\n",
    "workflow5.add_edge('create_vectorStore', 'create_retriever')\n",
    "workflow5.add_edge('create_retriever', 'get_answer')\n",
    "workflow5.add_edge('get_answer', 'return_answer')\n",
    "\n",
    "workflow5.set_entry_point(\"deciding_question_type\")\n",
    "workflow5.set_finish_point(\"return_answer\")\n",
    "app5 = workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app5.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# defining AgentState Object\n",
    "inputs = AgentState(\n",
    "    #question = \"Let me know about base field?\"\n",
    "    question = \"create a form for employee information\"\n",
    ")\n",
    "\n",
    "output = app5.invoke(inputs)\n",
    "\n",
    "if output[\"question_type\"] == \"creation\":\n",
    "    print(output[\"json_content\"])\n",
    "else:\n",
    "    print(output[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
