{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph, A very basic example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function3(input3):\n",
    "    return input3\n",
    "\n",
    "def function1(input1):\n",
    "    return input1 + \"from first function,\"\n",
    "\n",
    "def function2(input2):\n",
    "    output=function3(\"this is third in between,\")\n",
    "    return input2 + \" \" + output + \" and greetings from second function,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow1=Graph()\n",
    "\n",
    "workflow1.add_node(\"function1\", function1)\n",
    "workflow1.add_node(\"function2\",function2)\n",
    "\n",
    "workflow1.add_edge(\"function1\",\"function2\")\n",
    "\n",
    "workflow1.set_entry_point(\"function1\")\n",
    "workflow1.set_finish_point(\"function2\")\n",
    "\n",
    "app1=workflow1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app1.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi this is Zain from first function, this is third in between, and greetings from second function,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app1.invoke(\"hi this is Zain \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is output from function1\n",
      "_______\n",
      "hi this is Zain from first function,\n",
      "\n",
      "\n",
      "here is output from function2\n",
      "_______\n",
      "hi this is Zain from first function, this is third in between, and greetings from second function,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input=\"hi this is Zain \"\n",
    "\n",
    "for output in app1.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets create workflow with llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessagesState, StateGraph\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemorySaver\n\u001b[1;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m      8\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\mzain\\.conda\\envs\\genai\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mzain\\.conda\\envs\\genai\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:551\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    550\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[1;32mc:\\Users\\mzain\\.conda\\envs\\genai\\Lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callingLLM(state: MessagesState):\n",
    "    #print(f\"messages: {state[\"messages\"]}\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response=llm.invoke(state[\"messages\"]).content\n",
    "    #print(f\"response: {response}\")\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def textToUpperCase(state: MessagesState):\n",
    "    #print(f\"messages: {state[\"messages\"]}\")\n",
    "    upper_case=state[\"messages\"][-1].content.upper()\n",
    "    return {\"messages\": upper_case}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "workflow2=StateGraph(state_schema=MessagesState)\n",
    "\n",
    "workflow2.add_node(\"llm\",callingLLM)\n",
    "workflow2.add_node(\"upper_string\",textToUpperCase)\n",
    "workflow2.add_edge(\"llm\",\"upper_string\")\n",
    "\n",
    "workflow2.set_entry_point(\"llm\")\n",
    "workflow2.set_finish_point(\"upper_string\")\n",
    "\n",
    "app2=workflow2.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app2.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# res = app2.invoke({\"messages\": \"My name is Zain, What are you?\"}, config)\n",
    "# res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = app2.invoke({\"messages\": \"what is my name?\"}, config)\n",
    "# res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"My name is Zain, what is a name of first Pakistan prime minister? give me only name.\"\n",
    "\n",
    "for output in app2.stream({\"messages\": input}, config):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value[\"messages\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"what is my name? give me only name\"\n",
    "\n",
    "for output in app2.stream({\"messages\": input}, config):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value[\"messages\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating a RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the AgentState type\n",
    "class AgentState(TypedDict):\n",
    "    question : str\n",
    "    question_type : str\n",
    "    answer : str\n",
    "    documents : list[object]\n",
    "    vector_store : None\n",
    "    retriever : None\n",
    "    json_content : str\n",
    "    is_valid_json : bool\n",
    "    \n",
    "\n",
    "# defining load_document function\n",
    "def load_document(agentState: AgentState) -> AgentState: \n",
    "    print(\"---DOCUMENT LOAD STARTED---\")\n",
    "    # Specify the root directory where you want to search for PDF files\n",
    "    filePath = \"EcoDocs.pdf\"\n",
    "\n",
    "    # Initialize an empty list to store loaded documents\n",
    "    docs = []\n",
    "\n",
    "    # load a single PDF file\n",
    "    print(filePath)\n",
    "    pdf_loader = PyPDFLoader(filePath)\n",
    "    docs = pdf_loader.load()\n",
    "\n",
    "    agentState['documents'] = docs\n",
    "    print(\"---DOCUMENT LOAD COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining split_text function\n",
    "def split_text(agentState: AgentState) -> AgentState: \n",
    "    print(\"---TEXT SPLITTING STARTED---\")\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    documents = agentState['documents']\n",
    "    new_docs = text_splitter.split_documents(documents=documents)\n",
    "    # doc_strings = [doc.page_content for doc in new_docs]\n",
    "    agentState['documents'] = new_docs\n",
    "    print(\"---TEXT SPLITTING COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining create_vectorStore function\n",
    "def create_vectorStore(agentState: AgentState) -> AgentState: \n",
    "    print(\"---VECTOR DB CREATION STARTED---\")\n",
    "    documents = agentState['documents']\n",
    "\n",
    "    if os.path.exists(\"ecodoc_faissdb\"):\n",
    "        vector_store = FAISS.load_local(folder_path= \"ecodoc_faissdb\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        vector_store.save_local(\"ecodoc_faissdb\")\n",
    "    agentState['vector_store'] = vector_store\n",
    "    print(\"---VECTOR DB CREATION COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining create_retriever function\n",
    "def create_retriever(agentState: AgentState) -> AgentState:\n",
    "    print(\"---RETRIEVER STARTED---\") \n",
    "    vector_store = agentState['vector_store']\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    agentState['retriever'] = retriever\n",
    "    print(\"---RETRIEVER COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "# defining get_answer function\n",
    "def get_answer(agentState: AgentState) -> AgentState: \n",
    "    print(\"---LLM CALLING STARTED---\")\n",
    "    question = agentState['question']\n",
    "    documents = agentState['documents']\n",
    "    \n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. When asked a question, provide the most relevant information from the provided documents.\n",
    "    If the information is not available, respond that there is no such information in the system files,\n",
    "    and suggest contacting the helpdesk for further assistance. \n",
    "    Do not use any other information than given vector store to provide answers.  \n",
    "    documents: {documents}\n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  {\"documents\": lambda x: documents, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    generation = rag_chain.invoke(question)\n",
    "\n",
    "    agentState['answer'] = generation\n",
    "    print(\"---LLM CALLING COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow4 = StateGraph(AgentState)\n",
    "workflow4.add_node(\"load_document\", load_document)\n",
    "workflow4.add_node(\"split_text\", split_text)\n",
    "workflow4.add_node(\"create_vectorStore\", create_vectorStore)\n",
    "workflow4.add_node(\"create_retriever\", create_retriever)\n",
    "workflow4.add_node(\"get_answer\", get_answer)\n",
    "workflow4.add_edge('load_document', 'split_text')\n",
    "workflow4.add_edge('split_text', 'create_vectorStore')\n",
    "workflow4.add_edge('create_vectorStore', 'create_retriever')\n",
    "workflow4.add_edge('create_retriever', 'get_answer')\n",
    "workflow4.set_entry_point(\"load_document\")\n",
    "workflow4.set_finish_point(\"get_answer\")\n",
    "app4 = workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app4.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# defining AgentState Object\n",
    "inputs = AgentState(\n",
    "    question = \"Let me know about base field?\"\n",
    ")\n",
    "\n",
    "output = app4.invoke(inputs)\n",
    "\n",
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "# %pip install jsonschema\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def deciding_question_type(agentState: AgentState) -> AgentState:\n",
    "    print(\"---LLM CALLING TO DECIDE QUESTION TYPE STARTED---\")\n",
    "    question = agentState['question']\n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. When asked a question, you need to decide if it is question our user asking to create a\n",
    "    a form .\n",
    "\n",
    "    create form means user actually want to create a dynamic input form a page in the application.\n",
    "    following are examples of create form: \n",
    "    example 1: create a form for employee information\n",
    "    example 2: create a form where in can store crop field attributes\n",
    "    example 3: create a page for customer's review feedback\n",
    "\n",
    "    you need to decide which type of question user is asking and then return one of following enum and nothing else.\n",
    "    question\n",
    "    creation\n",
    "    \n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  {\"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    question_type = rag_chain.invoke(question)\n",
    "    agentState['question_type'] = question_type\n",
    "    print(\"---LLM CALLING TO DECIDE QUESTION TYPE COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n",
    "\n",
    "def get_form_fields(agentState: AgentState) -> AgentState:\n",
    "    print(\"---LLM CALLING TO GENERATE FIELD STARTED---\")\n",
    "    question = agentState['question']\n",
    "    template=\"\"\" \n",
    "    You are an intelligent assistant. user ask you to create a form for a specific purpose. you need to create columns\n",
    "    and their attributes related to the form user ask to create. you need to return a list of columns and their attributes\n",
    "    you need to generate your answer in json format. column types must be one of following \n",
    "    'string', 'integer', 'date', 'decimal'\n",
    "    return only json and nothing else.\n",
    "\n",
    "    following is an example of form fields in json format:\n",
    "\n",
    "    {{\n",
    "        'columns': [\n",
    "            {{\n",
    "                'columnName': 'Name',\n",
    "                'columnType': 'string',\n",
    "                'textLength': 50,\n",
    "                'required'   : true\n",
    "            }},\n",
    "            {{\n",
    "                'name': 'date_of_birth',\n",
    "                'type': 'date',\n",
    "                'required': false\n",
    "            }},\n",
    "            {{\n",
    "                'name': 'phone',\n",
    "                'type': 'integer',\n",
    "                'required': true\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain =  { \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "    json_content = rag_chain.invoke(question)\n",
    "\n",
    "    agentState['json_content'] = json_content\n",
    "    # print(json_content )\n",
    "    print(\"---LLM CALLING TO GENERATE FIELD COMPLETED---\")\n",
    "    return agentState\n",
    "\n",
    "\n",
    "\n",
    "def validate_json(agentState: AgentState) -> AgentState:\n",
    "    print(\"---VALIDATE JSON STARTED---\")\n",
    "    json_content = agentState['json_content']\n",
    "    # load json schema from file 'json-schema.json'\n",
    "    with open('json-schema.json') as f:\n",
    "        schema = json.load(f)\n",
    "    # validate json content against schema\n",
    "    try:\n",
    "        validate(instance=json.loads(json_content), schema=schema) \n",
    "        agentState['is_valid_json'] = True\n",
    "    except Exception as e:\n",
    "        agentState['json_content'] = \"Invalid JSON content\"\n",
    "        agentState['is_valid_json'] = False\n",
    "    print(f\"---VALIDATE JSON COMPLETED : {agentState['is_valid_json']}---\")\n",
    "    return agentState\n",
    "\n",
    "def decide_to_regenerate(agentState: AgentState) -> Literal[\"get_form_fields\", \"return_answer\"]:\n",
    "    is_valid_json = agentState['is_valid_json']\n",
    "    if is_valid_json:\n",
    "        return \"return_answer\"\n",
    "    else:\n",
    "        return \"get_form_fields\"\n",
    "\n",
    "def decide_to_reg(agentState: AgentState) -> Literal[\"load_document\", \"get_form_fields\"]:\n",
    "    question_type = agentState['question_type']\n",
    "    if question_type == \"question\":\n",
    "        return \"load_document\"\n",
    "    else:\n",
    "        return \"get_form_fields\"\n",
    "\n",
    "def return_answer(agentState: AgentState) -> AgentState:\n",
    "    return agentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow5 = StateGraph(AgentState)\n",
    "\n",
    "workflow5.add_node(\"load_document\", load_document)\n",
    "workflow5.add_node(\"split_text\", split_text)\n",
    "workflow5.add_node(\"create_vectorStore\", create_vectorStore)\n",
    "workflow5.add_node(\"create_retriever\", create_retriever)\n",
    "workflow5.add_node(\"get_answer\", get_answer)\n",
    "workflow5.add_node(\"deciding_question_type\", deciding_question_type)\n",
    "workflow5.add_node(\"get_form_fields\", get_form_fields)\n",
    "workflow5.add_node(\"validate_json\", validate_json)\n",
    "workflow5.add_node(\"decide_to_regenerate\", decide_to_regenerate)\n",
    "workflow5.add_node(\"return_answer\", return_answer)\n",
    "workflow5.add_node(\"decide_to_reg\", decide_to_reg)\n",
    "\n",
    "#workflow5.add_edge('deciding_question_type', 'decide_to_reg')\n",
    "workflow5.add_conditional_edges('deciding_question_type', decide_to_reg)\n",
    "# workflow5.add_conditional_edges('decide_to_reg', decide_to_reg)\n",
    "workflow5.add_edge('get_form_fields', 'validate_json')\n",
    "workflow5.add_conditional_edges('validate_json', decide_to_regenerate)\n",
    "workflow5.add_edge('load_document', 'split_text')\n",
    "workflow5.add_edge('split_text', 'create_vectorStore')\n",
    "workflow5.add_edge('create_vectorStore', 'create_retriever')\n",
    "workflow5.add_edge('create_retriever', 'get_answer')\n",
    "workflow5.add_edge('get_answer', 'return_answer')\n",
    "\n",
    "workflow5.set_entry_point(\"deciding_question_type\")\n",
    "workflow5.set_finish_point(\"return_answer\")\n",
    "app5 = workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app5.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# defining AgentState Object\n",
    "inputs = AgentState(\n",
    "    #question = \"Let me know about base field?\"\n",
    "    question = \"create a form for employee information\"\n",
    ")\n",
    "\n",
    "output = app5.invoke(inputs)\n",
    "\n",
    "if output[\"question_type\"] == \"creation\":\n",
    "    print(output[\"json_content\"])\n",
    "else:\n",
    "    print(output[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
